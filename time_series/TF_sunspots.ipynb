{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9cncyCHpOjLlPzgHdWhAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ant358/tensorflow_projects/blob/main/time_series/TF_sunspots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uapu18EmxGt"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.layers import Dense, LSTM, Lambda, Conv1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "def normalization(series):\n",
        "    min = np.min(series)      # 1. Normalization \n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    return series\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift = 1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w : w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter = ',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            sunspots.append(float(row[2]))\n",
        "            time_step.append(int(row[0]))\n",
        "\n",
        "    series = np.array(sunspots)\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    series = normalization(series)          # normalized\n",
        "\n",
        "    split_time = 3000\n",
        "\n",
        "    time_train = time[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "\n",
        "    x_train = series[:split_time]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_size = 1000\n",
        "\n",
        "    train_set = windowed_dataset(x_train,\n",
        "                                 window_size = window_size,\n",
        "                                 batch_size = batch_size,\n",
        "                                 shuffle_buffer = shuffle_size)\n",
        "    \n",
        "    validation_set = windowed_dataset(x_valid,\n",
        "                                      window_size = window_size,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle_buffer = shuffle_size)\n",
        "    \n",
        "    model = Sequential([\n",
        "                        Conv1D(70, kernel_size = 5,\n",
        "                                padding = 'causal',\n",
        "                                activation = 'relu',\n",
        "                                input_shape = [None, 1]),\n",
        "                        LSTM(64, return_sequences=True),\n",
        "                        LSTM(64, return_sequences=True),\n",
        "                        Dense(30, activation='relu'),\n",
        "                        Dense(10, activation='relu'),\n",
        "                        Dense(1),\n",
        "                        Lambda(lambda x: x*400)         # 2. Lambda used\n",
        "    ]) \n",
        "\n",
        "    optimizer = SGD(learning_rate=1e-5, momentum=0.9)\n",
        "    loss = Huber()\n",
        "\n",
        "    model.compile(optimizer = optimizer, loss = loss, metrics = ['mae'])\n",
        "\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                 save_weights_only = True,\n",
        "                                 save_best_only = True,\n",
        "                                 monitor = 'val_mae',\n",
        "                                 verbose = 1)\n",
        "    \n",
        "    model.fit(train_set,\n",
        "              validation_data = (validation_set),\n",
        "              epochs = 100,\n",
        "              callbacks = [checkpoint])\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"model.h5\")"
      ]
    }
  ]
}